{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import mglearn\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.pipeline import make_union, make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from nltk.tag import pos_tag, map_tag\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import TransformerMixin\n",
    "import re\n",
    "\n",
    "%matplotlib inline\n",
    "train = pd.read_csv(\"spooky-authors/train.zip\", index_col=['id'])\n",
    "test = pd.read_csv(\"spooky-authors/test.zip\", index_col=['id'])\n",
    "sample_submission = pd.read_csv(\"spooky-authors/sample_submission.zip\", index_col=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 2)\n",
      "(8392, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идеята е да ползвам LDA, за да открия n на брой теми. С малко късмет различните автори ще са писали по различни теми и това ще помогне за тяхното идентифициране."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Използвам LDA с 20 теми (няма смисъл от повече, резултата не се променя особено)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19579, 20)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=.15, max_features=10000)\n",
    "X = vectorizer.fit_transform(train.text)\n",
    "lda = LatentDirichletAllocation(n_components=20, \n",
    "                                learning_method=\"batch\", max_iter=15, random_state=0)\n",
    "topics = lda.fit_transform(X)\n",
    "topics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Първо пробвам да предрека автора само с генерираните теми. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_text(df):\n",
    "    return df[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = make_union(*[\n",
    "    make_pipeline(FunctionTransformer(get_text, validate=False), TfidfVectorizer(ngram_range=(1, 2), min_df=2,\n",
    "                                                                                 max_df=0.8, lowercase=False)),\n",
    "])\n",
    "\n",
    "lda_f = LatentDirichletAllocation(n_components=20,\n",
    "                                learning_method=\"batch\", max_iter=15, random_state=0)\n",
    "vectorizer_f = CountVectorizer(max_df=.15, max_features=10000)\n",
    "lda_features = make_union(*[\n",
    "    make_pipeline(FunctionTransformer(get_text, validate=False),\n",
    "                  vectorizer_f,\n",
    "                  lda_f)\n",
    "])\n",
    "tfidf_lda = make_union(*[\n",
    "    tfidf_vectorizer,\n",
    "    lda_features\n",
    "])\n",
    "\n",
    "pipeline_lda = make_pipeline(lda_features, MultinomialNB(alpha=0.01))\n",
    "pipeline_tfidf_lda = make_pipeline(tfidf_lda, MultinomialNB(alpha=0.01))\n",
    "pipeline_tfidf = make_pipeline(tfidf_vectorizer, MultinomialNB(alpha=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5513174   0.52129942  0.5394636 ]\n",
      "[-0.96128556 -0.99249138 -0.98830282]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(pipeline_lda, train, train.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(pipeline_lda, train, train.author,cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не е много впечатляващ резултат. Да видим какви теми намери LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "on            at            by            on            his           \n",
      "from          or            an            by            at            \n",
      "were          three         upon          day           on            \n",
      "they          this          this          house         eyes          \n",
      "through       about         or            morning       door          \n",
      "down          two           his           from          into          \n",
      "out           upon          at            next          up            \n",
      "by            time          from          this          from          \n",
      "up            five          one           him           room          \n",
      "moon          one           is            so            upon          \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "all           not           were          his           his           \n",
      "from          have          by            have          me            \n",
      "me            be            their         mr            old           \n",
      "by            would         his           been          on            \n",
      "its           could         at            is            him           \n",
      "were          been          these         him           from          \n",
      "earth         this          an            this          night         \n",
      "this          should        all           by            when          \n",
      "us            might         on            years         not           \n",
      "nature        his           one           at            they          \n",
      "\n",
      "\n",
      "topic 10      topic 11      topic 12      topic 13      topic 14      \n",
      "--------      --------      --------      --------      --------      \n",
      "at            on            we            an            you           \n",
      "all           or            our           nor           is            \n",
      "came          its           is            what          your          \n",
      "from          from          us            they          said          \n",
      "length        street        this          me            will          \n",
      "upon          were          be            like          are           \n",
      "by            at            not           neither       do            \n",
      "more          by            must          things        not           \n",
      "when          up            an            more          me            \n",
      "some          one           so            see           what          \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "mglearn.tools.print_topics(topics=range(15), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прави ми впечатление, че повечето теми съдържат главно стоп думи, които са популярни за всеки текст. Може би е добра идея да се премахнат. \n",
    "\n",
    "Така и така ще ги махаме направо и един стемер да ударим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "explore = train.copy()\n",
    "explore['no_stop'] = explore.text.apply(lambda s: \" \".join([stemmer.stem(w) for w in str(s).split() if w.lower() not in stopwords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_no_stop(df):\n",
    "    return df[\"no_stop\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lda_features_no_stop = make_union(*[\n",
    "    make_pipeline(FunctionTransformer(get_no_stop, validate=False),\n",
    "                  vectorizer_f,\n",
    "                  lda_f)\n",
    "])\n",
    "pipeline_lda_no_stop = make_pipeline(lda_features_no_stop, MultinomialNB(alpha=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.50903799  0.47931352  0.46329502]\n",
      "[-1.00826273 -1.02304674 -1.03234007]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(pipeline_lda_no_stop, explore, explore.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(pipeline_lda_no_stop, explore, explore.author,cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резултата се влоши. Да видим новите теми."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "night         time          would         one           upon          \n",
      "upon          power         could         would         may           \n",
      "wind          could         me            say           littl         \n",
      "sound         upon          one           could         found         \n",
      "pass          everi         time          matter        also          \n",
      "came          spent         english       fact          despair       \n",
      "time          him           mani          however       then          \n",
      "heard         natur         voic          even          turn          \n",
      "hour          whose         yet           mere          even          \n",
      "seem          first         seem          upon          everi         \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "an            upon          us            us            eye           \n",
      "like          one           water         year          look          \n",
      "thing         mr            made          day           me            \n",
      "one           appear        one           raymond       saw           \n",
      "get           yet           long          would         heard         \n",
      "ye            natur         so            time          could         \n",
      "god           great         and           three         window        \n",
      "got           man           everi         one           felt          \n",
      "kind          that          see           may           seem          \n",
      "know          it            bodi          five          sound         \n",
      "\n",
      "\n",
      "topic 10      topic 11      topic 12      topic 13      topic 14      \n",
      "--------      --------      --------      --------      --------      \n",
      "old           old           dream         de            you           \n",
      "one           like          west          said          said          \n",
      "men           never         life          this          word          \n",
      "came          man           everi         could         know          \n",
      "street        upon          less          much          me            \n",
      "day           earth         desir         the           one           \n",
      "hill          human         turn          far           friend        \n",
      "night         last          men           go            dear          \n",
      "thing         mani          imposs        great         well          \n",
      "man           black         still         state         he            \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(max_df=.15, max_features=10000)\n",
    "X = vectorizer.fit_transform(explore.no_stop)\n",
    "lda = LatentDirichletAllocation(n_components=20, \n",
    "                                learning_method=\"batch\", max_iter=15, random_state=0)\n",
    "lda.fit(X)\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "mglearn.tools.print_topics(topics=range(15), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега изглеждат по-добре, но резултата се влоши. Най-вероятно въпреки, че изглеждат по релевантни, темите сега са по-общи за авторите. \n",
    "\n",
    "Да видим как ще се справя в комбинация с Bag Of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.83011642  0.83588722  0.83264368]\n",
      "[-0.4305994  -0.42365256 -0.43169241]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(pipeline_tfidf_lda, train, train.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(pipeline_tfidf_lda, train, train.author,cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резултата отново не е впечатляващ. Да сравним с чист Bag Of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.83195466  0.83466135  0.83187739]\n",
      "[-0.42530307 -0.418245   -0.42500535]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(pipeline_tfidf, train, train.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(pipeline_tfidf, train, train.author,cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Няма подобрение. Дори е малко по-зле."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ще пробвам да добавя feature среден брой думи в изречение. Също така ще пробвам да преброя различните части на речта във всяко изречение, т.е. ще ползвам pos tagger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_average_words_per_sentence(sentences):\n",
    "        count_words = []\n",
    "        for sentence in sentences:\n",
    "            count_words.append(len(sentence.split(\" \")))\n",
    "        return sum(count_words) / len(count_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_average_number_of_words(df):\n",
    "    df_new = df.copy()\n",
    "    df_new[\"average_words_per_sentence\"] = df_new.apply(\n",
    "            lambda x: get_average_words_per_sentence(re.split(r\"[.?!]\", x[\"text\"])), axis=1)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TAGS = [\"ADJ\", \"ADP\", \"ADV\", \"CONJ\", \"DET\", \"NOUN\", \"NUM\", \"PRT\", \"PRON\", \"VERB\", \".\", \"X\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_all_tags(text):\n",
    "        counter = defaultdict(int)\n",
    "        tagged = nltk.pos_tag(nltk.word_tokenize(text), tagset=\"universal\")\n",
    "        for _, tag in tagged:\n",
    "            counter[tag] += 1\n",
    "        return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_pos_features(df, tags):\n",
    "    df_new = df.copy()\n",
    "    for tag in tags:\n",
    "        df_new[tag] = np.zeros(df_new.shape[0]).astype(int)\n",
    "        \n",
    "    for i, row in df_new.iterrows():\n",
    "        tags_counted = count_all_tags(row.text)\n",
    "        for tag, frequency in tags_counted.items():\n",
    "            df_new.set_value(i, tag, frequency)\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По принцип nltk.pos_tag тагва с много прецизни тагове, които са и много наброй. Реших, че е достатъчно да ползвам по-общи категории, затова ползвам tagset=\"universal\". В този съкратен тагсет повечето тагове са self-explanatory с изключение на \".\" и \"X\". С точката се тагва всякаква пунктоация, като например .,?! и тн. X са всички думи, които са неизвестни за тагера, например думи, които не са на английски език."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да видим как се справя модела с новите features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PRT</th>\n",
       "      <th>PRON</th>\n",
       "      <th>VERB</th>\n",
       "      <th>.</th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id26305</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17569</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id11008</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27763</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id12958</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text author  ADJ  ADP  \\\n",
       "id                                                                            \n",
       "id26305  This process, however, afforded me no means of...    EAP    2    5   \n",
       "id17569  It never once occurred to me that the fumbling...    HPL    1    1   \n",
       "id11008  In his left hand was a gold snuff box, from wh...    EAP    5    6   \n",
       "id27763  How lovely is spring As we looked from Windsor...    MWS    6    6   \n",
       "id12958  Finding nothing else, not even gold, the Super...    HPL    1    3   \n",
       "\n",
       "         ADV  CONJ  DET  NOUN  NUM  PRT  PRON  VERB  .  X  \n",
       "id                                                         \n",
       "id26305    3     1    6    10    0    2     5     7  7  0  \n",
       "id17569    2     0    2     2    0    1     2     3  1  0  \n",
       "id11008    1     0    6    10    0    1     3     4  5  0  \n",
       "id27763    2     2    2    10    0    0     1     5  4  0  \n",
       "id12958    4     1    2     7    0    0     4     5  4  0  "
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_tagged = add_pos_features(train, TAGS)\n",
    "test_pos_tagged = add_pos_features(test, TAGS)\n",
    "train_pos_tagged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PRT</th>\n",
       "      <th>PRON</th>\n",
       "      <th>VERB</th>\n",
       "      <th>.</th>\n",
       "      <th>X</th>\n",
       "      <th>average_words_per_sentence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id26305</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17569</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id11008</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27763</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>17.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id12958</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text author  ADJ  ADP  \\\n",
       "id                                                                            \n",
       "id26305  This process, however, afforded me no means of...    EAP    2    5   \n",
       "id17569  It never once occurred to me that the fumbling...    HPL    1    1   \n",
       "id11008  In his left hand was a gold snuff box, from wh...    EAP    5    6   \n",
       "id27763  How lovely is spring As we looked from Windsor...    MWS    6    6   \n",
       "id12958  Finding nothing else, not even gold, the Super...    HPL    1    3   \n",
       "\n",
       "         ADV  CONJ  DET  NOUN  NUM  PRT  PRON  VERB  .  X  \\\n",
       "id                                                          \n",
       "id26305    3     1    6    10    0    2     5     7  7  0   \n",
       "id17569    2     0    2     2    0    1     2     3  1  0   \n",
       "id11008    1     0    6    10    0    1     3     4  5  0   \n",
       "id27763    2     2    2    10    0    0     1     5  4  0   \n",
       "id12958    4     1    2     7    0    0     4     5  4  0   \n",
       "\n",
       "         average_words_per_sentence  \n",
       "id                                   \n",
       "id26305                        21.0  \n",
       "id17569                         7.5  \n",
       "id11008                        18.5  \n",
       "id27763                        17.5  \n",
       "id12958                        14.0  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pos_avg = add_average_number_of_words(train_pos_tagged)\n",
    "test_pos_avg = add_average_number_of_words(test_pos_tagged)\n",
    "train_pos_avg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_author_text(df):\n",
    "    return df.drop([\"author\", \"text\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf = make_union(*[\n",
    "    make_pipeline(FunctionTransformer(get_text, validate=False),\n",
    "                                  TfidfVectorizer(ngram_range=(1, 2), min_df=2, \n",
    "                                                              max_df=0.8, lowercase=False)),\n",
    "])\n",
    "pipeline_tfidf = make_pipeline(tfidf, MultinomialNB(alpha=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.83195466  0.83466135  0.83187739]\n",
      "[-0.42530307 -0.418245   -0.42500535]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(pipeline_tfidf, train_pos_avg.drop([\"author\"], axis=1), train_pos_tagged.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(pipeline_tfidf, train_pos_avg.drop([\"author\"], axis=1), train_pos_tagged.author,cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резултатът се влоши. Да видим какъв е резултата без tfidf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.52864583  0.52896108  0.53195402]\n",
      "[-0.99589232 -0.99010315 -0.98235638]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(MultinomialNB(alpha=0.01), train_pos_avg.drop([\"author\", \"text\"], axis=1), train_pos_tagged.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(MultinomialNB(alpha=0.01), train_pos_avg.drop([\"author\", \"text\"], axis=1), train_pos_tagged.author,cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.54718137  0.55194606  0.56076628]\n",
      "[-0.9443998  -0.93700313 -0.93107771]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(LogisticRegression(), train_pos_avg.drop([\"author\", \"text\"], axis=1), train_pos_tagged.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(LogisticRegression(), train_pos_avg.drop([\"author\", \"text\"], axis=1), train_pos_tagged.author,cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резултатът с логистична регресия е малко по-добър. \n",
    "\n",
    "Ще опитам да комбинирам LDA с пос таговете и средния брой думи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_avg = make_union(*[\n",
    "    lda_features,\n",
    "    make_pipeline(FunctionTransformer(drop_author_text, validate=False))\n",
    "])\n",
    "pipeline_lda_avg = make_pipeline(lda_avg, LogisticRegression(C=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.6395527   0.62886914  0.63019157]\n",
      "[-0.80370316 -0.84348148 -0.82907251]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(pipeline_lda_avg, train_pos_avg, train_pos_tagged.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(pipeline_lda_avg, train_pos_avg, train_pos_tagged.author,cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заедно все пак са по-добре от колкото отделно. Опитвам и с MultinomialNB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.5859375   0.57814894  0.57563218]\n",
      "[-0.90826364 -0.9333977  -0.92121947]\n"
     ]
    }
   ],
   "source": [
    "pipeline_lda_avg_nb = make_pipeline(lda_avg, MultinomialNB(alpha=0.01))\n",
    "print(cross_val_score(pipeline_lda_avg_nb, train_pos_tagged, train_pos_tagged.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(pipeline_lda_avg_nb, train_pos_tagged, train_pos_tagged.author,cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резултата е по зле."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зачетох се в тази [статия](http://blog.kaggle.com/2016/12/27/a-kagglers-guide-to-model-stacking-in-practice/) и реших да пробвам model stacking (макар и само с един модел). Идеята е да се сметнат вероятностите само с tfidf векторизиране и после те да се ползват като фийчъри. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stack_model(train, test, model, splits):\n",
    "    kf = StratifiedKFold(n_splits=splits, shuffle=True)\n",
    "    proba_train = pd.np.zeros([train.shape[0], 3])\n",
    "    proba_test = pd.np.zeros([test.shape[0], 3])\n",
    "    y = train.author\n",
    "    for train_indices, test_indices in kf.split(train, y):\n",
    "        split_train_x, split_train_y = train.iloc[train_indices], y.iloc[train_indices]\n",
    "        split_test_x =  train.iloc[test_indices]\n",
    "        \n",
    "        model.fit(split_train_x, split_train_y)\n",
    "        proba_train[test_indices] = model.predict_proba(split_test_x)\n",
    "        proba_test += model.predict_proba(test)\n",
    "    proba_test /= splits\n",
    "    train_df = pd.DataFrame(proba_train, columns=['EAP', 'HPL', 'MWS'], index=train.index)\n",
    "    test_df = pd.DataFrame(proba_test, columns=['EAP', 'HPL', 'MWS'], index=test.index)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_proba, test_proba = stack_model(train, test, pipeline_tfidf, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id26305</th>\n",
       "      <td>0.994668</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.004586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17569</th>\n",
       "      <td>0.599233</td>\n",
       "      <td>0.221939</td>\n",
       "      <td>0.178828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id11008</th>\n",
       "      <td>0.922528</td>\n",
       "      <td>0.077064</td>\n",
       "      <td>0.000407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27763</th>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.999360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id12958</th>\n",
       "      <td>0.340180</td>\n",
       "      <td>0.519122</td>\n",
       "      <td>0.140698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              EAP       HPL       MWS\n",
       "id                                   \n",
       "id26305  0.994668  0.000746  0.004586\n",
       "id17569  0.599233  0.221939  0.178828\n",
       "id11008  0.922528  0.077064  0.000407\n",
       "id27763  0.000387  0.000252  0.999360\n",
       "id12958  0.340180  0.519122  0.140698"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_proba.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_train = pd.concat([train_pos_avg, train_proba], axis=1)\n",
    "stacked_test = pd.concat([test_pos_avg, test_proba], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>author</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADP</th>\n",
       "      <th>ADV</th>\n",
       "      <th>CONJ</th>\n",
       "      <th>DET</th>\n",
       "      <th>NOUN</th>\n",
       "      <th>NUM</th>\n",
       "      <th>PRT</th>\n",
       "      <th>PRON</th>\n",
       "      <th>VERB</th>\n",
       "      <th>.</th>\n",
       "      <th>X</th>\n",
       "      <th>average_words_per_sentence</th>\n",
       "      <th>EAP</th>\n",
       "      <th>HPL</th>\n",
       "      <th>MWS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id26305</th>\n",
       "      <td>This process, however, afforded me no means of...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0.994668</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.004586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id17569</th>\n",
       "      <td>It never once occurred to me that the fumbling...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.599233</td>\n",
       "      <td>0.221939</td>\n",
       "      <td>0.178828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id11008</th>\n",
       "      <td>In his left hand was a gold snuff box, from wh...</td>\n",
       "      <td>EAP</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>18.5</td>\n",
       "      <td>0.922528</td>\n",
       "      <td>0.077064</td>\n",
       "      <td>0.000407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id27763</th>\n",
       "      <td>How lovely is spring As we looked from Windsor...</td>\n",
       "      <td>MWS</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>17.5</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000252</td>\n",
       "      <td>0.999360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id12958</th>\n",
       "      <td>Finding nothing else, not even gold, the Super...</td>\n",
       "      <td>HPL</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.340180</td>\n",
       "      <td>0.519122</td>\n",
       "      <td>0.140698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text author  ADJ  ADP  \\\n",
       "id                                                                            \n",
       "id26305  This process, however, afforded me no means of...    EAP    2    5   \n",
       "id17569  It never once occurred to me that the fumbling...    HPL    1    1   \n",
       "id11008  In his left hand was a gold snuff box, from wh...    EAP    5    6   \n",
       "id27763  How lovely is spring As we looked from Windsor...    MWS    6    6   \n",
       "id12958  Finding nothing else, not even gold, the Super...    HPL    1    3   \n",
       "\n",
       "         ADV  CONJ  DET  NOUN  NUM  PRT  PRON  VERB  .  X  \\\n",
       "id                                                          \n",
       "id26305    3     1    6    10    0    2     5     7  7  0   \n",
       "id17569    2     0    2     2    0    1     2     3  1  0   \n",
       "id11008    1     0    6    10    0    1     3     4  5  0   \n",
       "id27763    2     2    2    10    0    0     1     5  4  0   \n",
       "id12958    4     1    2     7    0    0     4     5  4  0   \n",
       "\n",
       "         average_words_per_sentence       EAP       HPL       MWS  \n",
       "id                                                                 \n",
       "id26305                        21.0  0.994668  0.000746  0.004586  \n",
       "id17569                         7.5  0.599233  0.221939  0.178828  \n",
       "id11008                        18.5  0.922528  0.077064  0.000407  \n",
       "id27763                        17.5  0.000387  0.000252  0.999360  \n",
       "id12958                        14.0  0.340180  0.519122  0.140698  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.85401348  0.85442844  0.8559387 ]\n",
      "[-0.40179466 -0.39357812 -0.39269627]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(LogisticRegression(C=100), stacked_train.drop([\"text\", \"author\"], axis=1), stacked_train.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(LogisticRegression(C=100), stacked_train.drop([\"text\", \"author\"], axis=1), stacked_train.author,cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Това подобри резултата!\n",
    "\n",
    "Пробвах няколко C-та на ръка, със 100 има най-добър резултат.\n",
    "\n",
    "Да видим с lda дали няма да се подобри."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.85278799  0.85688017  0.85471264]\n",
      "[-0.39859616 -0.39149199 -0.38997119]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(pipeline_lda_avg, stacked_train, stacked_train.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(pipeline_lda_avg, stacked_train, stacked_train.author,cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log_loss-а падна малко\n",
    "\n",
    "Последна идея за подобрение. От разгледаните фийчъри на лекции забелязах (след доста пробване), че unique_words дава най-добри резултати. Ще пробвам да го добавя."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stacked_train[\"unique_words\"] = stacked_train.text.apply(lambda s: len(set(str(s).split())))\n",
    "stacked_test[\"unique_words\"] = stacked_test.text.apply(lambda s: len(set(str(s).split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.85278799  0.8584125   0.8559387 ]\n",
      "[-0.3937938  -0.38883294 -0.38583781]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(pipeline_lda_avg, stacked_train, stacked_train.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(pipeline_lda_avg, stacked_train, stacked_train.author,cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Има малко подобрение. Време е за събмит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_text(df):\n",
    "    return df.drop([\"text\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_submit = make_union(*[\n",
    "    lda_features,\n",
    "    make_pipeline(FunctionTransformer(drop_text, validate=False))\n",
    "])\n",
    "pipeline_submit = make_pipeline(avg_submit, LogisticRegression(C=100))\n",
    "pipeline_submit.fit(stacked_train.drop([\"author\"], axis=1), stacked_train.author)\n",
    "prediction = pipeline_submit.predict_proba(stacked_test)\n",
    "submit_file = pd.DataFrame(prediction, columns=['EAP', 'HPL', 'MWS'], index=test.index)\n",
    "submit_file.head(10)\n",
    "submit_file.to_csv(\"~/Desktop/spooky_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резултата в кагъл е:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/spooky.png\"/>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
