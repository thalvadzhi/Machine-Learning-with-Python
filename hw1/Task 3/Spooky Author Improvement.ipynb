{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import mglearn\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "%matplotlib inline\n",
    "train = pd.read_csv(\"spooky-authors/train.zip\", index_col=['id'])\n",
    "test = pd.read_csv(\"spooky-authors/test.zip\", index_col=['id'])\n",
    "sample_submission = pd.read_csv(\"spooky-authors/sample_submission.zip\", index_col=['id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Първо събирам train и test сетовете. По този начин ако има думи, които ги има само в теста CountVectorizer създаде колони за тях и в трейна. Когато тренирам модел ще ползвам само редовете от които идват от train сета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combo = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19579, 2)\n",
      "(8392, 1)\n",
      "(27971, 2)\n"
     ]
    }
   ],
   "source": [
    "print(train.shape)\n",
    "print(test.shape)\n",
    "print(combo.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идеята е да ползвам LDA, за да открия n на брой теми. С малко късмет различните автори ще са писали по различни теми и това ще помогне за тяхното идентифициране."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Използвам LDA с 20 теми (няма смисъл от повече, резултата не се променя особено)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27971, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=.15, max_features=10000)\n",
    "X = vectorizer.fit_transform(combo.text)\n",
    "lda = LatentDirichletAllocation(n_components=20, \n",
    "                                learning_method=\"batch\", max_iter=15, random_state=0)\n",
    "topics = lda.fit_transform(X)\n",
    "topics.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Създавам си функции, които да извличат темите от техта и да го векторизират с TfidfVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize_text(text):\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=2,\n",
    "                                 max_df=0.8, lowercase=False)\n",
    "    return tfidf.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_topics(text, ld):\n",
    "    vectorizer = CountVectorizer(max_df=.15, max_features=10000)\n",
    "    vec = vectorizer.fit_transform(text)\n",
    "    return ld.transform(vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Събирам двете sparse матрици в една."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine_topics_and_vectors(vectors, topics):\n",
    "    sparse_topics = coo_matrix(topics)\n",
    "    return hstack([vectors, sparse_topics])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прилагам всичките операции чрез transform_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_text(text, lda):\n",
    "    vector = vectorize_text(text)\n",
    "    topics = generate_topics(text, lda)\n",
    "    return combine_topics_and_vectors(vector, topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Първо пробвам да предрека автора само с генерираните теми. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.40349265  0.40346307  0.4035249 ]\n",
      "[-1.08185797 -1.08186615 -1.08166839]\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB(alpha=0.01)\n",
    "text = generate_topics(combo[:19579].text, lda)\n",
    "print(cross_val_score(model, text, train.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(model, text, train.author,cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Да видим какви теми намери LDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "which         we            is            more          by            \n",
      "or            our           this          than          years         \n",
      "is            us            no            his           these         \n",
      "from          were          at            no            were          \n",
      "its           from          very          less          one           \n",
      "some          which         be            which         many          \n",
      "their         now           so            him           his           \n",
      "are           or            not           now           this          \n",
      "these         let           there         days          few           \n",
      "natural       by            an            found         are           \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "his           at            which         on            you           \n",
      "him           by            upon          from          me            \n",
      "me            which         one           its           have          \n",
      "from          were          is            which         do            \n",
      "at            an            by            by            not           \n",
      "when          from          this          where         what          \n",
      "then          his           be            out           can           \n",
      "which         this          from          into          are           \n",
      "would         length        or            through       know          \n",
      "himself       me            an            over          will          \n",
      "\n",
      "\n",
      "topic 10      topic 11      topic 12      topic 13      topic 14      \n",
      "--------      --------      --------      --------      --------      \n",
      "have          at            her           be            not           \n",
      "been          which         she           me            could         \n",
      "be            when          his           your          be            \n",
      "would         night         on            will          did           \n",
      "at            me            him           you           any           \n",
      "all           on            raymond       is            nor           \n",
      "which         before        heart         not           would         \n",
      "not           day           by            am            or            \n",
      "must          no            from          said          from          \n",
      "or            up            eyes          this          this          \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "mglearn.tools.print_topics(topics=range(15), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прави ми впечатление, че повечето теми съдържат главно стоп думи, които са популярни за всеки текст. Може би е добра идея да се премахнат. \n",
    "\n",
    "Така и така ще ги махаме направо и един стемер да ударим."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "explore = combo.copy()\n",
    "explore['no_stop'] = explore.text.apply(lambda s: \" \".join([stemmer.stem(w) for w in str(s).split() if w.lower() not in stopwords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27971, 20)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_explore = CountVectorizer(max_df=.15, max_features=10000)\n",
    "X_explore = vectorizer_explore.fit_transform(explore.no_stop)\n",
    "lda_explore = LatentDirichletAllocation(n_components=20, \n",
    "                                learning_method=\"batch\", max_iter=15, random_state=0)\n",
    "topics_explore = lda_explore.fit_transform(X_explore)\n",
    "topics_explore.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "went          upon          old           night         one           \n",
      "came          it            man           day           heard         \n",
      "town          arm           thing         de            someth        \n",
      "sound         lay           think         upon          could         \n",
      "wind          head          like          sun           wall          \n",
      "sea           west          mani          watch         hous          \n",
      "one           still         ever          wander        old           \n",
      "night         even          would         toward        ancient       \n",
      "time          two           dream         rest          it            \n",
      "sometim       saw           world         scene         told          \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "him           could         upon          said          day           \n",
      "one           upon          feet          me            murder        \n",
      "dr            human         hand          you           madam         \n",
      "great         word          right         let           would         \n",
      "too           mind          one           one           night         \n",
      "thing         face          side          would         first         \n",
      "old           yet           eye           friend        daughter      \n",
      "two           voic          could         us            third         \n",
      "year          everi         left          see           commit        \n",
      "busi          whose         head          must          far           \n",
      "\n",
      "\n",
      "topic 10      topic 11      topic 12      topic 13      topic 14      \n",
      "--------      --------      --------      --------      --------      \n",
      "would         me            an            mr            one           \n",
      "could         seem          know          upon          new           \n",
      "young         fear          knew          door          time          \n",
      "father        yet           thing         room          old           \n",
      "make          horror        ye            must          age           \n",
      "her           like          men           person        place         \n",
      "even          him           never         well          death         \n",
      "poor          sleep         like          time          certain       \n",
      "affect        dread         made          read          year          \n",
      "mind          thi           say           found         found         \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorting = np.argsort(lda_explore.components_, axis=1)[:, ::-1]\n",
    "feature_names = np.array(vectorizer_explore.get_feature_names())\n",
    "mglearn.tools.print_topics(topics=range(15), feature_names=feature_names, sorting=sorting, topics_per_chunk=5, n_words=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сега изглеждат по-добре. Има неща като murder, love."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.81541054  0.8210236   0.81333333]\n",
      "[-0.4707332  -0.45912487 -0.46660947]\n"
     ]
    }
   ],
   "source": [
    "model_explore = MultinomialNB(alpha=0.01)\n",
    "text_explore = transform_text(explore[:19579].no_stop, lda_explore)\n",
    "print(cross_val_score(model_explore, text_explore, train.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(model_explore, text_explore, train.author,cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Няма подобрение, дори резултата се влоши."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не се получи особено добре. Изглежда авторите не използват толкова различни теми (все пак всички са spooky :D)\n",
    "\n",
    "Пробвам само с Tfidf векторизиран текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.84987745  0.84400858  0.84061303]\n",
      "[-0.39747551 -0.39623287 -0.4030639 ]\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB(alpha=0.01)\n",
    "text = vectorize_text(combo[:19579].text)\n",
    "print(cross_val_score(model, text, train.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(model, text, train.author,cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Това вече е по-добър резултат от този на лекции. Изглежда комбинирането на двата файла оказва влияние.\n",
    "\n",
    "Да проверим все пак като се ползват и двете дали ще има полза. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.8504902   0.84462151  0.8410728 ]\n",
      "[-0.39707522 -0.39583773 -0.40274484]\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB(alpha=0.01)\n",
    "text = transform_text(combo[:19579].text, lda)\n",
    "print(cross_val_score(model, text, train.author, cv=3, n_jobs=3))\n",
    "print(cross_val_score(model, text, train.author, cv=3, n_jobs=3, \n",
    "                      scoring='neg_log_loss'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резултатите се подобриха съвсем минимално, но все пак се подобриха."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combo_vec = transform_text(combo.text, lda).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = MultinomialNB(alpha=0.01)\n",
    "train_vec = combo_vec[:19579, :]\n",
    "test_vec = combo_vec[19579:, :]\n",
    "model.fit(train_vec, train.author)\n",
    "test_predictions = model.predict_proba(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit_file = pd.DataFrame(test_predictions, columns=['EAP', 'HPL', 'MWS'], index=test.index)\n",
    "submit_file.head(10)\n",
    "submit_file.to_csv(\"~/Desktop/spooky_prediction.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резултата в kaggle e :\n",
    "<img src=\"img/spooky_kaggle.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Резултата без LDA в kaggle беше малко по нисък - 0.369.... така, че LDA все пак помогна малко.\n",
    "\n",
    "Също пробвах с LogisticRegression, SVC, RandomForest с много различни параметри, но винаги бяха значително по-слаби от Бейс, затова не ги включих в тетрадката."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
